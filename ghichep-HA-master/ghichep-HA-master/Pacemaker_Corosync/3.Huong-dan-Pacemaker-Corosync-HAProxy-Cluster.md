## Hướng dẫn cấu hình HAProxy Cluster bằng Pacemaker + Corosync

### Menu

[1. Chuẩn bị](#1)

[2. Các bước thực hiện](#2)

- [2.1 Cài đặt Apache Web-server](#2.1)
- [2.2 Cài đặt và cấu hình HAProxy](#2.2)
- [2.3 Cài đặt Pacemaker và Corosync](#2.3)
- [2.4 Cấu hình Cluster với Pacemaker và Corosync](#2.4)
- [2.5 Tạo cluster resource và đặt các thuộc tính] #2.5)
- [2.6 Test trường hợp fail-over bằng tay](#2.6)

[3. Tham khảo](#3)

## 1. Chuẩn bị <a name="1" />

- 2 server sử dụng OS CentOS
- Cấu hình hostname cho các server

Cụ thể:

**Node 1**

```
OS: CentOS 7 64 bit
Hostname: node1
IP: 192.168.100.196
Gateway: 192.168.100.1
Network: 192.168.100.0/24
```

**Node 2**

```
OS: CentOS 7 64 bit
Hostname: node2
IP: 192.168.100.197
Gateway: 192.168.100.1
Network: 192.168.100.0/24
```

#### Mô hình

<img src="http://image.prntscr.com/image/2378e8f54640479aaf9bab185c69aa76.png" width=75% />

- Trong mô hình, HAProxy làm nhiệm vụ phân phối các request từ phía người dùng vào các backend. Pacemaker và Corosync đảm nhiệm tạo một Virtual IP và quản lý các tài nguyên (HAProxy và VIP)

- Trước khi cài đặt, chúng ta phải cấu hình hostname cho mỗi node và ghi chúng vào `hosts`

```
[root@node1 ~] hostnamectl set-hostname node1
```

```
[root@node2 ~] hostnamectl set-hostname node2
```

- Ghi thêm vào `hosts` của mỗi server

```
vi /etc/hosts
```

```
[...]
192.168.100.196 node1
192.168.100.197 node2
```

## 2. Các bước thực hiện <a name="2" />

### 2.1 Cài đặt Apache Web-server <a name="2.1" />

Cài đặt và cấu hình Apache trên cả 2 node:

```
yum -y install httpd*
```

Đổi port hoạt động của `Apache`:

```
sed -i 's/Listen 80/Listen 8080/' /etc/httpd/conf/httpd.conf
```

Start `Apache` và cho nó khởi động cùng hệ thống

```
systemctl start httpd
systemctl enable httpd
```

### 2.2 Cài đặt và cấu hình HAProxy

Chúng ta cài đặt HAProxy ở trên cả 2 node như sau:

```
yum install -y haproxy
```

Sửa file cấu hình:

```
cp /etc/haproxy/haproxy.cfg /etc/haproxy/haproxy.cfg_orig
cat /dev/null > /etc/haproxy/haproxy.cfg
vi /etc/haproxy/haproxy.cfg
```

```
global
        log 127.0.0.1   local0
        log 127.0.0.1   local1 notice
        #log loghost    local0 info
        maxconn 4096
        #debug
        #quiet
        user haproxy
        group haproxy

defaults
        log     global
        mode    http
        option  httplog
        option  dontlognull
        retries 3
        redispatch
        maxconn 2000
        timeout connect  5000
        timeout client  10000
        timeout server  10000

listen webfarm 192.168.100.123:80 # Lang nghe tren VIP
       mode http
       stats enable
       stats auth admin:1
       balance roundrobin
       cookie JSESSIONID prefix
       option httpclose
       option forwardfor
       #option httpchk HEAD /check.txt HTTP/1.0
       server srv1 192.168.100.196:8080 cookie A check
       server srv2 192.168.100.197:8080 cookie B check
```

- **stats enable**: Bật tính năng Webmin của HAProxy, truy cập vào: http://192.168.100.123/haproxy?stats
- **admin:1**: là User và Password để đăng nhập vào webmin của HAProxy


Cấu hình tường lửa cho phép người dùng bên ngoài có thể truy cập vào qua port 80:

```
firewall-cmd --zone=public --add-port=80/tcp --permanent
firewall-cmd --reload
```

### 2.3 Cài đặt Pacemaker và Corosync <a name="2.3" />

Cài đặt các gói cluster trên cả 2 node bằng `yum`:

```
yum install -y pacemaker pcs fence-agents-all psmisc policycoreutils-python
```

Cũng như phần trên, chúng ta cấu hình tường lửa cho phép dịch vụ HA:

```
firewall-cmd --permanent --add-service=high-availability
firewall-cmd --reload
```

### 2.4 Cấu hình Cluster với Pacemaker và Corosync <a name="2.4" />

Trên cả 2 node, chúng ta đặt password cho user `hacluster` để xác thực với nhau, 2 mật khẩu phải trùng khớp

```
echo "redhat" | passwd --stdin hacluster
```

Tiếp theo, chúng ta khởi động dịch vụ trên cả 2 node:

```
systemctl start pcsd
systemctl enable pcsd
```

Trên node1, chúng ta xác thực 2 node với nhau bằng lệnh:

```
[root@node1 ~]# pcs cluster auth node1 node2 -u hacluster -p redhat
node1: Authorized
node2: Authorized
```

Sau khi xác thực, chúng ta tạo 1 cluster trên node 1 có tên là `mycluster` để chúng có thể tạo và đồng bộ các file cấu hình với nhau.

```
[root@node1 ~]# pcs cluster setup --name mycluster node1 node2
Shutting down pacemaker/corosync services...
Redirecting to /bin/systemctl stop  pacemaker.service
Redirecting to /bin/systemctl stop  corosync.service
Killing any remaining services...
Removing all cluster configuration files...
node1: Succeeded
node2: Succeeded
Synchronizing pcsd certificates on nodes node1, node2...
node1: Success
node2: Success
Restaring pcsd on the nodes in order to reload the certificates...
node1: Success
node2: Success
```

Khởi động và kích hoạt cluster mới tạo trên node 1 bằng lệnh:

```
[root@node1 ~]# pcs cluster start --all
[root@node1 ~]# pcs cluster enable --all
```

Xem lại trạng thái của cluster trên các node:

```
pcs status
```

Tắt Quorum và STONITH, cho các tài nguyên hoạt động trên cùng 1 node

```
pcs property set stonith-enabled=false
pcs property set no-quorum-policy=ignore
pcs property set default-resource-stickiness="INFINITY"
```

### 2.5 Tạo cluster resource và đặt các thuộc tính <a name="2.5" />

Tiếp đến, chúng ta tạo ra các tài nguyên như VIP, HAproxy trên node1 để khi có yêu cầu sẽ bring up nó lên trên các node khác trong trường hợp có sự cố xảy ra:

- Tạo cho VIP

```
[root@node1 ~]# pcs resource create VirtIP ocf:heartbeat:IPaddr2 ip=192.168.100.123 cidr_netmask=32 op monitor interval=30s
```

- Tạo cho HAProxy

Để khai báo một tài nguyên thuộc `systemd`, chúng ta tạm thời tắt `node2` đi bằng lệnh

```
[root@node1 ~]# pcs cluster stop node2
```

Tạo một resource trên thuộc `systemd`:

```
[root@node1 ~]# pcs resource create HAproxy systemd:haproxy op monitor interval=5s
```

Kiểm tra lại trạng thái của các tài nguyên:

```
[root@node1 ~]# pcs status resources
 VirtIP (ocf::heartbeat:IPaddr2):       Started node1
 HAproxy        (systemd:haproxy):      Started node1
```

Start lại `node2` vừa tắt ở bên trên:

```
[root@node1 ~]# pcs cluster start node2
```

Mặc định thì thời gian timeout cho start, stop và monitor các tài nguyên là 20s. Chúng ta chỉnh lại thời gian này bằng lệnh:

```
[root@node1 ~]# pcs resource op defaults timeout=240s
[root@node1 ~]# pcs resource op defaults
timeout: 240s
```

Bây giờ, chúng ta có thể truy cập vào địa chỉ VIP: 192.168.100.123 để có thể kiểm tra sự hoạt động của cluster.

### 2.6 Test trường hợp fail-over bằng tay <a name="2.6" />

Hiện tại, tất cả các tài nguyên được sử dụng trên node1. Hãy stop chúng lại để tạo ra trường hợp fail-over và sang node2 kiểm tra các tài nguyên:

```
[root@node1 ~]# pcs cluster stop node1
node1: Stopping Cluster (pacemaker)...
node1: Stopping Cluster (corosync)...
```

Khi `node1` đã stop, chúng ta sang `node2` để kiểm tra:

```
[root@node2 ~]# pcs status resources

Full list of resources:

VirtIP (ocf::heartbeat:IPaddr2):       Started node2
HAproxy        (systemd:haproxy):      Started node2

```

Như vậy là chúng ta đã hoàn thành quá trình cài đặt cluster bằng Pacemaker và Corosync.

## 3. Tham khảo <a name="3" />
- https://github.com/hoangdh/ghichep-HA/tree/master/HAProxy
- https://apuntesderootblog.wordpress.com/2014/08/26/clustered-haproxy-for-load-balancing-web-sites/

